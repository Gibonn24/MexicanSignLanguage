{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibonn24/MexicanSignLanguage/blob/main/Proyecto_Final_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f192134",
      "metadata": {
        "id": "0f192134"
      },
      "source": [
        "# Traductor de Lenguaje de Se√±as a Texto\n",
        "\n",
        "**Proyecto Final ‚Äì Machine Learning**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Integrantes\n",
        "| Nombre | % de contribuci√≥n |\n",
        "|--------|-------------------|\n",
        "| Giordano Fuentes | 100% |\n",
        "\n",
        "> Ajusta la tabla seg√∫n corresponda."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577a0873",
      "metadata": {
        "id": "577a0873"
      },
      "source": [
        "## 2. Introducci√≥n\n",
        "\n",
        "\n",
        "> La comunicaci√≥n entre personas sordas y oyentes sigue siendo una barrera. Este proyecto busca traducir autom√°ticamente videos de Lengua de Se√±as a texto en espa√±ol, usando aprendizaje profundo y visi√≥n computacional, para facilitar la inclusi√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d75dbc8",
      "metadata": {
        "id": "6d75dbc8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "25b7f28b",
      "metadata": {
        "id": "25b7f28b"
      },
      "outputs": [],
      "source": [
        "#Se encuentra en (\"./notebooks/EDA_dynamics.ipynb\") y (\"./notebooks/EDA_letters.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd42751",
      "metadata": {
        "id": "cbd42751"
      },
      "source": [
        "## 4. Metodolog√≠a\n",
        "Describe la arquitectura general:\n",
        "1. **Extracci√≥n de caracter√≠sticas** con un modelo preentrenado (p.ej. *I3D* / *S3D*) usando [`video_features`](https://github.com/v-iashin/video_features).\n",
        "2. **Modelo de traducci√≥n** secuencia‚Äìa‚Äìsecuencia (GRU/Transformer) que mapea embeddings de video ‚Üí texto (glosas o frases).\n",
        "3. **P√©rdida** CTC o CrossEntropy seg√∫n alineaci√≥n.\n",
        "\n",
        "Incluye un diagrama opcional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d5cebc69",
      "metadata": {
        "id": "d5cebc69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce GTX 1660 Ti'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "from utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from pyprojroot import here\n",
        "from pathlib import Path\n",
        "import random\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "458cffd2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyAV: 12.2.0\n",
            "TorchVision: 0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import av, torchvision, sys, importlib.metadata\n",
        "print(\"PyAV:\", av.__version__)              # deber√≠a mostrar 14.4.0\n",
        "print(\"TorchVision:\", torchvision.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d87eba88",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Documents\\ML\\venv\\Lib\\site-packages\\torchvision\\io\\video.py:169: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([57, 900, 900, 3]) {'video_fps': 30.0}\n"
          ]
        }
      ],
      "source": [
        "from torchvision.io import read_video\n",
        "rgb, _, info = read_video(\"C:/Users/User/Documents/ML/data/letters/dynamics/J/S1-J-perfil-1.mp4\")\n",
        "print(rgb.shape, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1a73980e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\moabitcoin_ig65m-pytorch_master\n"
          ]
        }
      ],
      "source": [
        "from omegaconf import OmegaConf\n",
        "from utils.utils import build_cfg_path\n",
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "\n",
        "# Cargar config base\n",
        "args = OmegaConf.load(build_cfg_path(\"r21d\"))\n",
        "args.feature_type     = \"r21d\"\n",
        "args.model_name       = \"r2plus1d_34_8_ig65m_ft_kinetics\"\n",
        "args.stack_size       = 8\n",
        "args.step_size        = 8\n",
        "args.extraction_fps   = 15          # normaliza todos los v√≠deos\n",
        "args.tmp_path         = \"tmp\"\n",
        "args.output_path      = \"feats\"\n",
        "args.on_extraction    = \"return\"    # o 'save_numpy'\n",
        "args.device           = \"cuda:0\"    # o 'cpu'\n",
        "args.show_pred        = False\n",
        "\n",
        "extractor = ExtractR21D(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e6134662",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "class CSVDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_path, transform=None, class_to_idx=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Si no se pasa mapeo externo, lo construye con las etiquetas del CSV\n",
        "        if class_to_idx is None:\n",
        "            classes = sorted(self.data[\"label\"].unique())\n",
        "            class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.class_to_idx[row[\"label\"]]\n",
        "        return img, label\n",
        "    \n",
        "all_labels = pd.concat([\n",
        "    pd.read_csv(\"letter_labels.csv\")['label'],\n",
        "    pd.read_csv(\"dynamics_videos.csv\")['label']\n",
        "]).unique()\n",
        "\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(sorted(all_labels))}\n",
        "\n",
        "# Transformaciones igual que antes\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.CenterCrop(112),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.43216,0.39466,0.37645],\n",
        "                         [0.22803,0.22145,0.21698]),\n",
        "])\n",
        "\n",
        "ds_static  = CSVDataset(\"letter_labels.csv\",  tfm,   class_to_idx)\n",
        "# 1. Split reproducible 80/10/10\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "N = len(ds_static)\n",
        "train_len = int(0.8*N); val_len = int(0.1*N); test_len = N - train_len - val_len\n",
        "\n",
        "train_s, val_s, test_s = random_split(\n",
        "    ds_static, [train_len, val_len, test_len],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# 2. DataLoaders\n",
        "dl_train = DataLoader(train_s, batch_size=64, shuffle=True, num_workers=0)\n",
        "dl_val   = DataLoader(val_s,   batch_size=64, shuffle=False, num_workers=0)\n",
        "dl_test  = DataLoader(test_s,  batch_size=64, shuffle=False, num_workers=0)\n",
        "# Modelo ResNet adaptado\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "# Cargar modelo preentrenado y adaptarlo\n",
        "model_img = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "model_img.fc = nn.Linear(model_img.fc.in_features, 27)\n",
        "model_img = model_img.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "03d15eaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoCSVDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset que lee rutas de v√≠deo y etiquetas desde un CSV y\n",
        "    extrae las caracter√≠sticas (embeddings) con un extractor 3D-CNN.\n",
        "\n",
        "    El CSV debe tener al menos dos columnas:\n",
        "        video_path,label\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, extractor, class_to_idx):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.extractor = extractor\n",
        "        self.class_to_idx = class_to_idx     # guardar mapeo externo\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        csv_path : str\n",
        "            Ruta al archivo CSV (`video_path,label`).\n",
        "        extractor : callable\n",
        "            Objeto con un m√©todo `.extract(path)[\"r21d\"]` que devuelve\n",
        "            un ndarray (n_stacks, 512) por v√≠deo.\n",
        "        class_to_idx : dict\n",
        "            Diccionario compartido con el mismo mapeo letra ‚Üí √≠ndice que\n",
        "            usas en el dataset de im√°genes est√°ticas.\n",
        "        \"\"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        feats = self.extractor.extract(row[\"video_path\"])[\"r21d\"]  # (stacks,512)\n",
        "        feats = torch.tensor(feats, dtype=torch.float32).mean(0)\n",
        "        label = self.class_to_idx[row[\"label\"]]\n",
        "        return feats, label\n",
        "\n",
        "# 0. Instancia del extractor R21D (ya lo tienes)\n",
        "ds_dynamic = VideoCSVDataset(\"dynamics_videos.csv\", extractor, class_to_idx)\n",
        "\n",
        "# 1. Split\n",
        "M = len(ds_dynamic)\n",
        "tr_len = int(0.8*M); va_len = int(0.1*M); te_len = M - tr_len - va_len\n",
        "\n",
        "video_train, video_val, video_test = random_split(\n",
        "    ds_dynamic, [tr_len, va_len, te_len],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# 2. DataLoaders\n",
        "dl_vtrain = DataLoader(video_train, batch_size=16, shuffle=True, num_workers=0)\n",
        "dl_vval   = DataLoader(video_val,   batch_size=16, shuffle=False, num_workers=0)\n",
        "dl_vtest  = DataLoader(video_test,  batch_size=16, shuffle=False, num_workers=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f715fcd",
      "metadata": {
        "id": "9f715fcd"
      },
      "source": [
        "## 5. Implementaci√≥n\n",
        "- Framework: **PyTorch**\n",
        "- Semilla de reproducibilidad: `42`\n",
        "- Enlace a notebook/Colab: <colab_link>\n",
        "\n",
        "Describe cualquier optimizaci√≥n o t√©cnica especial (e.g., *gradient clipping*, *mixed precision*, *early stopping*)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c007b95",
      "metadata": {},
      "source": [
        "## Entrenamiento de imagenes estaticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a15e6dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 01:  15%|‚ñà‚ñç        | 155/1041 [02:05<12:45,  1.16batch/s, loss=0.019] "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_img.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model_img.train()\n",
        "    running_loss = 0\n",
        "    pbar = tqdm(dl_train, desc=f\"Epoch {epoch+1:02d}\", unit=\"batch\")\n",
        "\n",
        "    for imgs, labels in pbar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model_img(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Validaci√≥n\n",
        "    model_img.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dl_val:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model_img(imgs)\n",
        "            val_loss += criterion(out, labels).item() * imgs.size(0)\n",
        "            pred = out.argmax(1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"üìä Epoch {epoch+1:02d} | train_loss={running_loss/len(train_s):.4f} | \"\n",
        "          f\"val_loss={val_loss/len(val_s):.4f} | val_acc={correct/total:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ccef19",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "class DynClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Espera tensores (B, 512) ‚Üí logits (B, n_classes).\n",
        "    Aplica media temporal antes de la fc.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=512, n_classes=n_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feats):             # feats: (B, stacks, 512)\n",
        "        x = feats.mean(1)                 # pooling temporal\n",
        "        return self.fc(x)\n",
        "\n",
        "model_vid = DynClassifier().to(device)\n",
        "\n",
        "opt_v = torch.optim.Adam(model_vid.parameters(), lr=1e-4)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    model_vid.train()\n",
        "    running_loss = 0\n",
        "    pbar = tqdm(dl_vtrain, desc=f\"Video Epoch {epoch+1:02d}\", unit=\"batch\")\n",
        "\n",
        "    for feats, labels in pbar:\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        opt_v.zero_grad()\n",
        "        out = model_vid(feats)\n",
        "        loss = crit(out, labels)\n",
        "        loss.backward()\n",
        "        opt_v.step()\n",
        "        running_loss += loss.item() * feats.size(0)\n",
        "\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Evaluaci√≥n\n",
        "    model_vid.eval()\n",
        "    correct = total = val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for feats, labels in dl_vval:\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            out = model_vid(feats)\n",
        "            val_loss += crit(out, labels).item() * feats.size(0)\n",
        "            preds = out.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"üé• Epoch {epoch+1:02d} | train_loss={running_loss/len(video_train):.4f} | \"\n",
        "          f\"val_loss={val_loss/len(video_val):.4f} | val_acc={correct/total:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdb8f75",
      "metadata": {
        "id": "3fdb8f75"
      },
      "source": [
        "## 6. Experimentaci√≥n\n",
        "Presenta las configuraciones de entrenamiento y resultados. Usa tablas o gr√°ficos (matplotlib) para loss y accuracy por √©poca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcf4071",
      "metadata": {
        "id": "0dcf4071"
      },
      "outputs": [],
      "source": [
        "img_path = \"data/letters/statics/G/img_0123.jpg\"\n",
        "img = tfm(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "pred = model_img(img).argmax(1).item()\n",
        "print(\"Predicci√≥n imagen:\", list(class_to_idx.keys())[pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197efbe3",
      "metadata": {},
      "outputs": [],
      "source": [
        "vid = \"data/letters/dynamics/J/S1-J-perfil-1.mp4\"\n",
        "feats = extractor.extract(vid)[\"r21d\"]          # (stacks,512)\n",
        "out = model_vid(torch.tensor(feats).unsqueeze(0).to(device))\n",
        "pred = out.argmax(1).item()\n",
        "print(\"Predicci√≥n v√≠deo :\", list(class_to_idx.keys())[pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745b1c95",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import json, pandas as pd\n",
        "import torch\n",
        "\n",
        "def eval_model(model, dataloader, name):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            if X.dim() == 3:          # v√≠deos ‚Üí (B, stacks, 512)\n",
        "                out = model(X)        # DynClassifier\n",
        "            else:                     # im√°genes ‚Üí (B, 3, 112, 112)\n",
        "                out = model(X)        # ResNet18\n",
        "\n",
        "            preds = out.argmax(1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    cm  = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # ----- guardar -----\n",
        "    metrics_path = f\"{name}_metrics.json\"\n",
        "    conf_path    = f\"{name}_confusion.csv\"\n",
        "\n",
        "    with open(metrics_path, \"w\") as fp:\n",
        "        json.dump({\"accuracy\": acc, \"macro_f1\": f1}, fp, indent=2)\n",
        "\n",
        "    pd.DataFrame(cm, dtype=int).to_csv(conf_path, index=False, header=False)\n",
        "\n",
        "    print(f\"\\n[{name.upper()}]  accuracy={acc:.3f}  macro-F1={f1:.3f}\")\n",
        "    print(f\"Matriz de confusi√≥n guardada en  {conf_path}\")\n",
        "    print(f\"M√©tricas guardadas en            {metrics_path}\")\n",
        "\n",
        "# --------- evaluaci√≥n -----------\n",
        "eval_model(model_img, dl_test,  \"static\")     # im√°genes\n",
        "eval_model(model_vid, dl_vtest, \"dynamic\")    # v√≠deos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7421b08",
      "metadata": {
        "id": "a7421b08"
      },
      "source": [
        "## 7. Discusi√≥n\n",
        "Analiza los resultados: ¬øqu√© patrones encuentras? ¬øQu√© gestos resultaron dif√≠ciles? ¬øC√≥mo influy√≥ la iluminaci√≥n o el background?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4859993",
      "metadata": {
        "id": "d4859993"
      },
      "source": [
        "## 8. Conclusiones\n",
        "Resume los hallazgos m√°s relevantes y menciona posibles mejoras futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88596832",
      "metadata": {
        "id": "88596832"
      },
      "source": [
        "## 9. Declaraci√≥n de Contribuci√≥n\n",
        "Describe el aporte de cada miembro del equipo con porcentajes de tiempo/actividad."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
