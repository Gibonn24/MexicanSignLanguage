{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibonn24/MexicanSignLanguage/blob/main/Proyecto_Final_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f192134",
      "metadata": {
        "id": "0f192134"
      },
      "source": [
        "# Traductor de Lenguaje de Señas a Texto\n",
        "\n",
        "**Proyecto Final – Machine Learning**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Integrantes\n",
        "| Nombre | % de contribución |\n",
        "|--------|-------------------|\n",
        "| Giordano Fuentes | 100% |\n",
        "\n",
        "> Ajusta la tabla según corresponda."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577a0873",
      "metadata": {
        "id": "577a0873"
      },
      "source": [
        "## 2. Introducción\n",
        "\n",
        "\n",
        "> La comunicación entre personas sordas y oyentes sigue siendo una barrera. Este proyecto busca traducir automáticamente videos de Lengua de Señas a texto en español, usando aprendizaje profundo y visión computacional, para facilitar la inclusión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d75dbc8",
      "metadata": {
        "id": "6d75dbc8"
      },
      "source": [
        "## 3. Dataset\n",
        "Explica qué dataset(s) usarás (p.ej. **WLASL**, **RWTH-PHOENIX-Weather**). Incluye detalles de tamaño, clases, formato de anotaciones y cualquier preprocesamiento.\n",
        "\n",
        "```text\n",
        "- Descarga original: <enlace>\n",
        "- Nº de videos seleccionados: TBD\n",
        "- Resolución típica: 256×256\n",
        "- FPS: 25\n",
        "```\n",
        "\n",
        "### 3.1 Exploración rápida del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b7f28b",
      "metadata": {
        "id": "25b7f28b"
      },
      "outputs": [],
      "source": [
        "# Ejemplo: inspeccionar algunas anotaciones\n",
        "import json, random, os\n",
        "dataset_path = '/path/to/WLASL'\n",
        "sample_json = '/path/to/WLASL/wlasl.json'\n",
        "with open(sample_json) as f:\n",
        "    data = json.load(f)\n",
        "print('Total samples:', len(data))\n",
        "for sample in random.sample(data, 3):\n",
        "    print(sample['gloss'], sample['video_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd42751",
      "metadata": {
        "id": "cbd42751"
      },
      "source": [
        "## 4. Metodología\n",
        "Describe la arquitectura general:\n",
        "1. **Extracción de características** con un modelo preentrenado (p.ej. *I3D* / *S3D*) usando [`video_features`](https://github.com/v-iashin/video_features).\n",
        "2. **Modelo de traducción** secuencia–a–secuencia (GRU/Transformer) que mapea embeddings de video → texto (glosas o frases).\n",
        "3. **Pérdida** CTC o CrossEntropy según alineación.\n",
        "\n",
        "Incluye un diagrama opcional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d5cebc69",
      "metadata": {
        "id": "d5cebc69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce GTX 1660 Ti'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "from utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a73980e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "from utils.utils import build_cfg_path\n",
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "\n",
        "# Cargar config base\n",
        "args = OmegaConf.load(build_cfg_path(\"r21d\"))\n",
        "args.feature_type     = \"r21d\"\n",
        "args.model_name       = \"r2plus1d_34_8_ig65m_ft_kinetics\"\n",
        "args.stack_size       = 8\n",
        "args.step_size        = 8\n",
        "args.extraction_fps   = 15          # normaliza todos los vídeos\n",
        "args.tmp_path         = \"tmp\"\n",
        "args.output_path      = \"feats\"\n",
        "args.on_extraction    = \"return\"    # o 'save_numpy'\n",
        "args.device           = \"cuda:0\"    # o 'cpu'\n",
        "args.show_pred        = False\n",
        "\n",
        "extractor = ExtractR21D(args)\n",
        "\n",
        "feats = extractor.extract(\"clip_2s.mp4\")\n",
        "print(feats[\"r21d\"].shape)  # (≈ n_stacks, 512)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f715fcd",
      "metadata": {
        "id": "9f715fcd"
      },
      "source": [
        "## 5. Implementación\n",
        "- Framework: **PyTorch**\n",
        "- Semilla de reproducibilidad: `42`\n",
        "- Enlace a notebook/Colab: <colab_link>\n",
        "\n",
        "Describe cualquier optimización o técnica especial (e.g., *gradient clipping*, *mixed precision*, *early stopping*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06be6a0",
      "metadata": {
        "id": "b06be6a0"
      },
      "outputs": [],
      "source": [
        "# 5.1 Configuración básica y semilla\n",
        "import torch, random, numpy as np\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdb8f75",
      "metadata": {
        "id": "3fdb8f75"
      },
      "source": [
        "## 6. Experimentación\n",
        "Presenta las configuraciones de entrenamiento y resultados. Usa tablas o gráficos (matplotlib) para loss y accuracy por época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcf4071",
      "metadata": {
        "id": "0dcf4071"
      },
      "outputs": [],
      "source": [
        "# 6.1 Training loop (simplificado)\n",
        "# Define model, optimizer, criterion...\n",
        "# for epoch in range(num_epochs):\n",
        "#     train_loss = ...\n",
        "#     val_loss = ...\n",
        "#     print(f'Epoch {epoch}: train {train_loss:.4f}, val {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7421b08",
      "metadata": {
        "id": "a7421b08"
      },
      "source": [
        "## 7. Discusión\n",
        "Analiza los resultados: ¿qué patrones encuentras? ¿Qué gestos resultaron difíciles? ¿Cómo influyó la iluminación o el background?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4859993",
      "metadata": {
        "id": "d4859993"
      },
      "source": [
        "## 8. Conclusiones\n",
        "Resume los hallazgos más relevantes y menciona posibles mejoras futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88596832",
      "metadata": {
        "id": "88596832"
      },
      "source": [
        "## 9. Declaración de Contribución\n",
        "Describe el aporte de cada miembro del equipo con porcentajes de tiempo/actividad."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
