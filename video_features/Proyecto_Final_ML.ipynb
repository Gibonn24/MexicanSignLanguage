{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibonn24/MexicanSignLanguage/blob/main/Proyecto_Final_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f192134",
      "metadata": {
        "id": "0f192134"
      },
      "source": [
        "# Traductor de Lenguaje de Señas a Texto\n",
        "\n",
        "**Proyecto Final – Machine Learning**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Integrantes\n",
        "| Nombre | % de contribución |\n",
        "|--------|-------------------|\n",
        "| Giordano Fuentes | 100% |\n",
        "\n",
        "> Ajusta la tabla según corresponda."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577a0873",
      "metadata": {
        "id": "577a0873"
      },
      "source": [
        "## 2. Introducción\n",
        "\n",
        "\n",
        "> La comunicación entre personas sordas y oyentes sigue siendo una barrera. Este proyecto busca traducir automáticamente videos de Lengua de Señas a texto en español, usando aprendizaje profundo y visión computacional, para facilitar la inclusión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d75dbc8",
      "metadata": {
        "id": "6d75dbc8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b7f28b",
      "metadata": {
        "id": "25b7f28b"
      },
      "outputs": [],
      "source": [
        "#Se encuentra en (\"./notebooks/EDA_dynamics.ipynb\") y (\"./notebooks/EDA_letters.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd42751",
      "metadata": {
        "id": "cbd42751"
      },
      "source": [
        "## 4. Metodología\n",
        "Describe la arquitectura general:\n",
        "1. **Extracción de características** con un modelo preentrenado (p.ej. *I3D* / *S3D*) usando [`video_features`](https://github.com/v-iashin/video_features).\n",
        "2. **Modelo de traducción** secuencia–a–secuencia (GRU/Transformer) que mapea embeddings de video → texto (glosas o frases).\n",
        "3. **Pérdida** CTC o CrossEntropy según alineación.\n",
        "\n",
        "Incluye un diagrama opcional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d5cebc69",
      "metadata": {
        "id": "d5cebc69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce GTX 1660 Ti'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "from utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from pyprojroot import here\n",
        "from pathlib import Path\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "458cffd2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyAV: 12.2.0\n",
            "TorchVision: 0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import av, torchvision, sys, importlib.metadata\n",
        "print(\"PyAV:\", av.__version__)              # debería mostrar 14.4.0\n",
        "print(\"TorchVision:\", torchvision.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d87eba88",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Documents\\ML\\venv\\Lib\\site-packages\\torchvision\\io\\video.py:169: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([57, 900, 900, 3]) {'video_fps': 30.0}\n"
          ]
        }
      ],
      "source": [
        "from torchvision.io import read_video\n",
        "rgb, _, info = read_video(\"C:/Users/User/Documents/ML/data/letters/dynamics/J/S1-J-perfil-1.mp4\")\n",
        "print(rgb.shape, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1a73980e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\moabitcoin_ig65m-pytorch_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 512)\n"
          ]
        }
      ],
      "source": [
        "from omegaconf import OmegaConf\n",
        "from utils.utils import build_cfg_path\n",
        "from models.r21d.extract_r21d import ExtractR21D\n",
        "\n",
        "# Cargar config base\n",
        "args = OmegaConf.load(build_cfg_path(\"r21d\"))\n",
        "args.feature_type     = \"r21d\"\n",
        "args.model_name       = \"r2plus1d_34_8_ig65m_ft_kinetics\"\n",
        "args.stack_size       = 8\n",
        "args.step_size        = 8\n",
        "args.extraction_fps   = 15          # normaliza todos los vídeos\n",
        "args.tmp_path         = \"tmp\"\n",
        "args.output_path      = \"feats\"\n",
        "args.on_extraction    = \"return\"    # o 'save_numpy'\n",
        "args.device           = \"cuda:0\"    # o 'cpu'\n",
        "args.show_pred        = False\n",
        "\n",
        "extractor = ExtractR21D(args)\n",
        "\n",
        "\n",
        "feats = extractor.extract(\"./data/letters/dynamics/J/S1-J-perfil-1.mp4\")\n",
        "print(feats[\"r21d\"].shape)  # (≈ n_stacks, 512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6134662",
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'ImageFolder' from 'torch.utils.data' (c:\\Users\\User\\Documents\\ML\\venv\\Lib\\site-packages\\torch\\utils\\data\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageFolder, DataLoader\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m tfm = transforms.Compose([\n\u001b[32m      7\u001b[39m     transforms.Resize(\u001b[32m128\u001b[39m),\n\u001b[32m      8\u001b[39m     transforms.CenterCrop(\u001b[32m112\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m                          [\u001b[32m0.22803\u001b[39m,\u001b[32m0.22145\u001b[39m,\u001b[32m0.21698\u001b[39m]),\n\u001b[32m     12\u001b[39m ])\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'ImageFolder' from 'torch.utils.data' (c:\\Users\\User\\Documents\\ML\\venv\\Lib\\site-packages\\torch\\utils\\data\\__init__.py)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(self.data['label'].unique())\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img = Image.open(row['image_path']).convert('RGB')\n",
        "        label = self.class_to_idx[row['label']]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "    \n",
        "all_labels = pd.concat([\n",
        "    pd.read_csv(\"letter_labels.csv\")['label'],\n",
        "    pd.read_csv(\"dynamics_videos.csv\")['label']\n",
        "]).unique()\n",
        "\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(sorted(all_labels))}\n",
        "\n",
        "# Transformaciones igual que antes\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.CenterCrop(112),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.43216,0.39466,0.37645],\n",
        "                         [0.22803,0.22145,0.21698]),\n",
        "])\n",
        "\n",
        "ds_static  = CSVDataset(\"letter_labels.csv\",  tfm,   class_to_idx)\n",
        "# 1. Split reproducible 80/10/10\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "N = len(ds_static)\n",
        "train_len = int(0.8*N); val_len = int(0.1*N); test_len = N - train_len - val_len\n",
        "\n",
        "train_s, val_s, test_s = random_split(\n",
        "    ds_static, [train_len, val_len, test_len],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# 2. DataLoaders\n",
        "dl_train = DataLoader(train_s, batch_size=64, shuffle=True, num_workers=4)\n",
        "dl_val   = DataLoader(val_s,   batch_size=64, shuffle=False, num_workers=4)\n",
        "dl_test  = DataLoader(test_s,  batch_size=64, shuffle=False, num_workers=4)\n",
        "# Modelo ResNet adaptado\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "# Cargar modelo preentrenado y adaptarlo\n",
        "model_img = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "model_img.fc = nn.Linear(model.fc.in_features, 27)\n",
        "model_img = model_img.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d15eaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoCSVDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset que lee rutas de vídeo y etiquetas desde un CSV y\n",
        "    extrae las características (embeddings) con un extractor 3D-CNN.\n",
        "\n",
        "    El CSV debe tener al menos dos columnas:\n",
        "        video_path,label\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path: str, extractor, class_to_idx: dict):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        csv_path : str\n",
        "            Ruta al archivo CSV (`video_path,label`).\n",
        "        extractor : callable\n",
        "            Objeto con un método `.extract(path)[\"r21d\"]` que devuelve\n",
        "            un ndarray (n_stacks, 512) por vídeo.\n",
        "        class_to_idx : dict\n",
        "            Diccionario compartido con el mismo mapeo letra → índice que\n",
        "            usas en el dataset de imágenes estáticas.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.extractor = extractor\n",
        "        self.class_to_idx = class_to_idx         # ← mapeo único y consistente\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        video_path = row['video_path']\n",
        "        label = self.class_to_idx[row['label']]\n",
        "\n",
        "        # Extraer embeddings (n_stacks, 512)\n",
        "        feats = self.extractor.extract(video_path)[\"r21d\"]\n",
        "\n",
        "        # Devuelve tensor float32 y etiqueta int\n",
        "        return torch.tensor(feats, dtype=torch.float32), label\n",
        "\n",
        "# 0. Instancia del extractor R21D (ya lo tienes)\n",
        "ds_dynamic = VideoCSVDataset(\"dynamics_videos.csv\", extractor, class_to_idx)\n",
        "\n",
        "# 1. Split\n",
        "M = len(ds_dynamic)\n",
        "tr_len = int(0.8*M); va_len = int(0.1*M); te_len = M - tr_len - va_len\n",
        "\n",
        "video_train, video_val, video_test = random_split(\n",
        "    ds_dynamic, [tr_len, va_len, te_len],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# 2. DataLoaders\n",
        "dl_vtrain = DataLoader(video_train, batch_size=16, shuffle=True)\n",
        "dl_vval   = DataLoader(video_val,   batch_size=16, shuffle=False)\n",
        "dl_vtest  = DataLoader(video_test,  batch_size=16, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f715fcd",
      "metadata": {
        "id": "9f715fcd"
      },
      "source": [
        "## 5. Implementación\n",
        "- Framework: **PyTorch**\n",
        "- Semilla de reproducibilidad: `42`\n",
        "- Enlace a notebook/Colab: <colab_link>\n",
        "\n",
        "Describe cualquier optimización o técnica especial (e.g., *gradient clipping*, *mixed precision*, *early stopping*)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c007b95",
      "metadata": {},
      "source": [
        "## Entrenamiento de imagenes estaticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a15e6dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_img.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model_img.train()\n",
        "    running = 0\n",
        "    for imgs, labels in dl_train:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model_img(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item()*imgs.size(0)\n",
        "\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    model_img.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dl_val:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model_img(imgs)\n",
        "            val_loss += criterion(out, labels).item()*imgs.size(0)\n",
        "            pred = out.argmax(1)\n",
        "            correct += (pred==labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    print(f\"Epoch {epoch+1:02d}\"\n",
        "          f\"  train_loss={running/len(train_s):.4f}\"\n",
        "          f\"  val_loss={val_loss/len(val_s):.4f}\"\n",
        "          f\"  val_acc={correct/total:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ccef19",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DynClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Espera tensores (B, 512) → logits (B, n_classes).\n",
        "    Aplica media temporal antes de la fc.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=512, n_classes=n_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feats):             # feats: (B, stacks, 512)\n",
        "        x = feats.mean(1)                 # pooling temporal\n",
        "        return self.fc(x)\n",
        "\n",
        "model_vid = DynClassifier().to(device)\n",
        "\n",
        "opt_v = torch.optim.Adam(model_vid.parameters(), lr=1e-4)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    model_vid.train()\n",
        "    for feats, labels in dl_vtrain:\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        opt_v.zero_grad()\n",
        "        out = model_vid(feats)\n",
        "        loss = crit(out, labels)\n",
        "        loss.backward()\n",
        "        opt_v.step()\n",
        "\n",
        "    # eval\n",
        "    model_vid.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for feats, labels in dl_vval:\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            preds = model_vid(feats).argmax(1)\n",
        "            correct += (preds==labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    print(f\"Epoch {epoch+1:02d}  val_acc={correct/total:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdb8f75",
      "metadata": {
        "id": "3fdb8f75"
      },
      "source": [
        "## 6. Experimentación\n",
        "Presenta las configuraciones de entrenamiento y resultados. Usa tablas o gráficos (matplotlib) para loss y accuracy por época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcf4071",
      "metadata": {
        "id": "0dcf4071"
      },
      "outputs": [],
      "source": [
        "img_path = \"data/letters/statics/G/img_0123.jpg\"\n",
        "img = tfm(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "pred = model_img(img).argmax(1).item()\n",
        "print(\"Predicción imagen:\", list(class_to_idx.keys())[pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197efbe3",
      "metadata": {},
      "outputs": [],
      "source": [
        "vid = \"data/letters/dynamics/J/S1-J-perfil-1.mp4\"\n",
        "feats = extractor.extract(vid)[\"r21d\"]          # (stacks,512)\n",
        "out = model_vid(torch.tensor(feats).unsqueeze(0).to(device))\n",
        "pred = out.argmax(1).item()\n",
        "print(\"Predicción vídeo :\", lis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7421b08",
      "metadata": {
        "id": "a7421b08"
      },
      "source": [
        "## 7. Discusión\n",
        "Analiza los resultados: ¿qué patrones encuentras? ¿Qué gestos resultaron difíciles? ¿Cómo influyó la iluminación o el background?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4859993",
      "metadata": {
        "id": "d4859993"
      },
      "source": [
        "## 8. Conclusiones\n",
        "Resume los hallazgos más relevantes y menciona posibles mejoras futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88596832",
      "metadata": {
        "id": "88596832"
      },
      "source": [
        "## 9. Declaración de Contribución\n",
        "Describe el aporte de cada miembro del equipo con porcentajes de tiempo/actividad."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
