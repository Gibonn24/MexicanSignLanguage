{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibonn24/MexicanSignLanguage/blob/main/Proyecto_Final_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f192134",
      "metadata": {
        "id": "0f192134"
      },
      "source": [
        "# Traductor de Lenguaje de Señas a Texto\n",
        "\n",
        "**Proyecto Final – Machine Learning**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Integrantes\n",
        "| Nombre | % de contribución |\n",
        "|--------|-------------------|\n",
        "| Giordano Fuentes | 100% |\n",
        "\n",
        "> Ajusta la tabla según corresponda."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577a0873",
      "metadata": {
        "id": "577a0873"
      },
      "source": [
        "## 2. Introducción\n",
        "\n",
        "\n",
        "> La comunicación entre personas sordas y oyentes sigue siendo una barrera. Este proyecto busca traducir automáticamente videos de Lengua de Señas a texto en español, usando aprendizaje profundo y visión computacional, para facilitar la inclusión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d75dbc8",
      "metadata": {
        "id": "6d75dbc8"
      },
      "source": [
        "## 3. Dataset\n",
        "Explica qué dataset(s) usarás (p.ej. **WLASL**, **RWTH-PHOENIX-Weather**). Incluye detalles de tamaño, clases, formato de anotaciones y cualquier preprocesamiento.\n",
        "\n",
        "```text\n",
        "- Descarga original: <enlace>\n",
        "- Nº de videos seleccionados: TBD\n",
        "- Resolución típica: 256×256\n",
        "- FPS: 25\n",
        "```\n",
        "\n",
        "### 3.1 Exploración rápida del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b7f28b",
      "metadata": {
        "id": "25b7f28b"
      },
      "outputs": [],
      "source": [
        "# Ejemplo: inspeccionar algunas anotaciones\n",
        "import json, random, os\n",
        "dataset_path = '/path/to/WLASL'\n",
        "sample_json = '/path/to/WLASL/wlasl.json'\n",
        "with open(sample_json) as f:\n",
        "    data = json.load(f)\n",
        "print('Total samples:', len(data))\n",
        "for sample in random.sample(data, 3):\n",
        "    print(sample['gloss'], sample['video_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd42751",
      "metadata": {
        "id": "cbd42751"
      },
      "source": [
        "## 4. Metodología\n",
        "Describe la arquitectura general:\n",
        "1. **Extracción de características** con un modelo preentrenado (p.ej. *I3D* / *S3D*) usando [`video_features`](https://github.com/v-iashin/video_features).\n",
        "2. **Modelo de traducción** secuencia–a–secuencia (GRU/Transformer) que mapea embeddings de video → texto (glosas o frases).\n",
        "3. **Pérdida** CTC o CrossEntropy según alineación.\n",
        "\n",
        "Incluye un diagrama opcional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cebc69",
      "metadata": {
        "id": "d5cebc69"
      },
      "outputs": [],
      "source": [
        "# 4.1 Extracción de embeddings (placeholder)\n",
        "# !pip install video-features\n",
        "from video_features.extractor import FeatureExtractor\n",
        "\n",
        "extractor = FeatureExtractor(\n",
        "    file_with_video_paths='videos.txt',\n",
        "    batch_size=8,\n",
        "    feature_model_name='i3d_flow'\n",
        ")\n",
        "extractor.run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f715fcd",
      "metadata": {
        "id": "9f715fcd"
      },
      "source": [
        "## 5. Implementación\n",
        "- Framework: **PyTorch**\n",
        "- Semilla de reproducibilidad: `42`\n",
        "- Enlace a notebook/Colab: <colab_link>\n",
        "\n",
        "Describe cualquier optimización o técnica especial (e.g., *gradient clipping*, *mixed precision*, *early stopping*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06be6a0",
      "metadata": {
        "id": "b06be6a0"
      },
      "outputs": [],
      "source": [
        "# 5.1 Configuración básica y semilla\n",
        "import torch, random, numpy as np\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdb8f75",
      "metadata": {
        "id": "3fdb8f75"
      },
      "source": [
        "## 6. Experimentación\n",
        "Presenta las configuraciones de entrenamiento y resultados. Usa tablas o gráficos (matplotlib) para loss y accuracy por época."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcf4071",
      "metadata": {
        "id": "0dcf4071"
      },
      "outputs": [],
      "source": [
        "# 6.1 Training loop (simplificado)\n",
        "# Define model, optimizer, criterion...\n",
        "# for epoch in range(num_epochs):\n",
        "#     train_loss = ...\n",
        "#     val_loss = ...\n",
        "#     print(f'Epoch {epoch}: train {train_loss:.4f}, val {val_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7421b08",
      "metadata": {
        "id": "a7421b08"
      },
      "source": [
        "## 7. Discusión\n",
        "Analiza los resultados: ¿qué patrones encuentras? ¿Qué gestos resultaron difíciles? ¿Cómo influyó la iluminación o el background?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4859993",
      "metadata": {
        "id": "d4859993"
      },
      "source": [
        "## 8. Conclusiones\n",
        "Resume los hallazgos más relevantes y menciona posibles mejoras futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88596832",
      "metadata": {
        "id": "88596832"
      },
      "source": [
        "## 9. Declaración de Contribución\n",
        "Describe el aporte de cada miembro del equipo con porcentajes de tiempo/actividad."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}